---
permalink: "/ko/research/Perception/"
layout: ko/research
lang: ko
title: "인식"
description: "다양한 입력을 통해 사용자와 주변 상황을 정확하게 인식하고 이해에 기반하여 종합적으로 판단하는 능력은 중요합니다."
date: 2019-10-03
weight: 3
header_transparent: true
fa_icon: false
icon: "assets/images/icons/icons8-color-palette-100.png"
thumbnail: "/assets/images/gen/research/research-3-thumbnail.webp"
image: "/assets/images/gen/research/research-3.webp"

hero:
  enabled: true
  heading: "인식"
  sub_heading: "다양한 입력을 통해 사용자와 주변 상황을 정확하게 인식하고 이해에 기반하여 종합적으로 판단하는 능력은 중요합니다."
  text_color: "#ffffff"
  background_color: ""
  background_gradient: true
  background_image_blend_mode: "overlay" # "overlay", "multiply", "screen"
  background_image: "/assets/images/gen/research/research-3.webp"
  fullscreen_mobile: false
  fullscreen_desktop: false
  height: 660px
  buttons:
    enabled: false
    list:
      - text: "Buy Now"
        url: "https://www.zerostatic.io/theme/jekyll-advance/"
        external: true
        fa_icon: false
        size: large
        outline: false
        style: "primary"
---

# 인식

Robotic systems can process images and video far faster than humans. Using that advantage in “인식” has paved the way for breakthroughs in security, surveillance and related activities. 
Pairing that 인식 with manipulation enables robots to become “butlers” that can interpret and organize a cluttered environment for helping an aging population tidy their homes and accomplish other tasks that will help them continue to live independently. 

## How does this work? 

Understanding what the robot sees in the world is a significant challenge. 
The environmental lighting conditions vary, motion in the world is very complex, and there is a huge amount of data to process in the video. 
인식 research investigates mathematical models and advanced computer algorithms to perceive the world. 
Researchers in RGA Inc. investigate both physical models of 인식 as well as data-driven, machine learning models.

{% include framework/shortcodes/figure.html src="/assets/images/gen/content/content-1.webp" title="Steve Francia" caption="Designing in Figma" alt="Photo of designing a website in Figma" link="https://figma.com" target="_blank" %}

Computerized “deep neural networks,” modeled after human brain activity, have become proficient at perceiving and identifying commonly occurring activities, including walking and running people.
They work less well, however, for uncommon activities, such as people falling on a sidewalk near the street. 
RGA researchers are working on technologies that will allow machines to do what comes naturally to people, who have an exceptional ability to react appropriately to new phenomena with few or no previous examples and little explanation to work from.

To do so, computers must process and “understand” huge amounts of data quickly and efficiently. 
To understand video content, researchers have formulated methods for processing pixels within volumes of space and time rather than through a sequence of frames. 
This leads to a more difficult modeling and computational problem, but yields more robust output.

Robotics also needs to improve at another set of skills humans pick up naturally, as infants: physical manipulation. 
RGA engineers have developed a Fetch robot that has learned how to pick up objects and move them around based on a simple demonstration from a human. 
But for the Fetch robot to do more fully mobile manipulation, it will need to see the world around it much better than it does now. 
It will need to know what it can do with every object it sees, whether it is the handle of a door or cup, each having its own function. 

But what if a robot tried to do something more complicated, like making breakfast? 
The robot would need to reason through all the different actions it would need to take with various objects. 
And all of these actions form a chain that together that can achieve a larger task. 
The next generation of robots will need to attain a level of reasoning that allows them to understand and reliably respond to the new objects that it sees in the real world.


![Design In Figma](/assets/images/gen/content/content-2.webp)
